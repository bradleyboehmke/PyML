
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Feature &amp; Target Engineering &#8212; Machine Learning with Python</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Content in Jupyter Book" href="../supervised_learning/content.html" />
    <link rel="prev" title="Modeling Process" href="modeling_process.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning with Python</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Machine Learning with Python
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Fundamentals
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="modeling_process.html">
   Modeling Process
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Feature &amp; Target Engineering
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Supervised Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../supervised_learning/content.html">
   Content in Jupyter Book
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Dimension Reduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../dimension_reduction/content.html">
   Content in Jupyter Book
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Clustering
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../clustering/content.html">
   Content in Jupyter Book
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Deep Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../deep_learning/content.html">
   Content in Jupyter Book
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/fundamentals/feature_eng.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/bradleyboehmke/PyML"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/bradleyboehmke/PyML/issues/new?title=Issue%20on%20page%20%2Ffundamentals/feature_eng.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/bradleyboehmke/PyML/edit/master/book/_build/fundamentals/feature_eng.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/bradleyboehmke/PyML/master?urlpath=tree/book/_build/fundamentals/feature_eng.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Feature &amp; Target Engineering
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-objectives">
   Learning objectives
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prerequisites">
   Prerequisites
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#target-engineering">
   Target engineering
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dealing-with-missingness">
   Dealing with missingness
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visualizing-missing-values">
     Visualizing missing values
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#imputation">
     Imputation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#estimated-statistic">
       Estimated statistic
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#k-nearest-neighbor">
       K-nearest neighbor
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#feature-filtering">
   Feature filtering
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#numeric-feature-engineering">
   Numeric feature engineering
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#categorical-feature-engineering">
   Categorical feature engineering
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dimension-reduction">
   Dimension reduction
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#proper-implementation">
   Proper implementation
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="feature-target-engineering">
<h1>Feature &amp; Target Engineering<a class="headerlink" href="#feature-target-engineering" title="Permalink to this headline">¶</a></h1>
<p>Data pre-processing and engineering techniques generally refer to the addition, deletion, or transformation of data.  The time spent on identifying data engineering needs can be significant and requires you to spend substantial time understanding your data…or as Leo Breiman said “live with your data before you plunge into modeling” <span id="id1">[<a class="reference internal" href="#id23">B+01</a>]</span>. Although this book primarily focuses on applying machine learning algorithms, feature engineering can make or break an algorithm’s predictive ability and deserves your continued focus and education.</p>
<p>We will not cover all the potential ways of implementing feature engineering; however, we’ll cover several fundamental pre-processing tasks that has the potential to significantly improve modeling performance. Moreover, different models have different sensitivities to the type of target and feature values in the model and we will try to highlight some of these concerns. For more in depth coverage of feature engineering, please refer to <span id="id2">[<a class="reference internal" href="#id27">KJ19</a>]</span> and <span id="id3">[<a class="reference internal" href="#id28">ZC18</a>]</span>.</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="learning-objectives">
<h1>Learning objectives<a class="headerlink" href="#learning-objectives" title="Permalink to this headline">¶</a></h1>
<p>By the end of this module you will know:</p>
<ul class="simple">
<li><p>When and how to transform the response variable (“target engineering”).</p></li>
<li><p>How to identify and deal with missing values.</p></li>
<li><p>When to filter unnecessary features.</p></li>
<li><p>Common ways to transform numeric and categorical features.</p></li>
<li><p>How to apply dimension reduction.</p></li>
<li><p>How to properly combine multiple pre-processing steps into the modeling process.</p></li>
</ul>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="prerequisites">
<h1>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Helper packages</span>
<span class="kn">import</span> <span class="nn">missingno</span> <span class="k">as</span> <span class="nn">msno</span>
<span class="kn">import</span> <span class="nn">modeldata</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">plotnine</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Modeling pre-processing with scikit-learn functionality</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">TransformedTargetRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">ColumnTransformer</span>
<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">make_column_selector</span> <span class="k">as</span> <span class="n">selector</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PowerTransformer</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">KNNImputer</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">VarianceThreshold</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="c1"># Modeling pre-processing with non-scikit-learn packages</span>
<span class="kn">from</span> <span class="nn">category_encoders.ordinal</span> <span class="kn">import</span> <span class="n">OrdinalEncoder</span>
<span class="kn">from</span> <span class="nn">feature_engine.encoding</span> <span class="kn">import</span> <span class="n">RareLabelEncoder</span>

<span class="c1"># Modeling</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Ames housing data</span>
<span class="n">ames</span> <span class="o">=</span> <span class="n">modeldata</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;ames&quot;</span><span class="p">)</span>

<span class="c1"># create train/test split</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">ames</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="c1"># separate features from labels and only use numeric features</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;Sale_Price&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train</span><span class="p">[[</span><span class="s2">&quot;Sale_Price&quot;</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="target-engineering">
<h1>Target engineering<a class="headerlink" href="#target-engineering" title="Permalink to this headline">¶</a></h1>
<p>Although not always a requirement, transforming the response variable can lead to predictive improvement, especially with parametric models which require that certain assumptions about the model be met. For instance, ordinary linear regression models assume that the prediction errors (and hence the response) are normally distributed. This is usually fine, except when the prediction target has heavy tails (i.e., <em>outliers</em>) or is skewed in one direction or the other. In these cases, the normality assumption likely does not hold. For example, as we saw in the data splitting section in the last module, the response variable for the Ames housing data (<code class="docutils literal notranslate"><span class="pre">Sale_Price</span></code>) is right (or positively) skewed ranging from $12,789 to $755,000. A simple linear model, say <span class="math notranslate nohighlight">\(\text{Sale_Price}=\beta_{0} + \beta_{1} \text{Year_Built} + \epsilon\)</span>, often assumes the error term <span class="math notranslate nohighlight">\(\epsilon\)</span> (and hence <code class="docutils literal notranslate"><span class="pre">Sale_Price</span></code>) is normally distributed; fortunately, a simple log (or similar) transformation of the response can often help alleviate this concern as the below plot illustrates.</p>
<p><img alt="" src="../_images/engineering-skewed-residuals-1.png" /></p>
<p>Furthermore, using a log (or other) transformation to minimize the response skewness can be used for shaping the business problem as well.  For example, in the <a class="reference external" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">House Prices: Advanced Regression Techniques Kaggle competition</a>, which used the Ames housing data, the competition focused on using a log transformed Sale Price response  because <em>“…taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.”</em> This would be an alternative to using the root mean squared logarithmic error (RMSLE) loss function as discussed in the last module.</p>
<p>There are three common approaches to help correct for positively skewed target variables:</p>
<ol class="simple">
<li><p>Normalize with a log transformation. This will transform most right skewed distributions to be approximately normal.</p></li>
<li><p>If your response has negative values or zeros then a log transformation will produce <code class="docutils literal notranslate"><span class="pre">NaN</span></code>s and <code class="docutils literal notranslate"><span class="pre">-Inf</span></code>s, respectively (you cannot take the logarithm of a negative number).  If the nonpositive response values are small (say between -0.99 and 0) then you can apply a small offset such as in <code class="docutils literal notranslate"><span class="pre">numpy.log1p()</span></code> which adds 1 to the value prior to applying a log transformation. If your data consists of values <span class="math notranslate nohighlight">\(\le -1\)</span>, use the Yeo-Johnson transformation instead.</p></li>
<li><p>(<strong>Preferred</strong>) Use a <em>Box Cox transformation</em>. A Box Cox transformation is more flexible than (but also includes as a special case) the log transformation and will find an appropriate transformation from a family of power transforms that will transform the variable as close as possible to a normal distribution <span id="id4">[<a class="reference internal" href="#id29">BC64</a>, <a class="reference internal" href="#id30">CR81</a>]</span>. At the core of the Box Cox transformation is an exponent, lambda (<span class="math notranslate nohighlight">\(\lambda\)</span>), which varies from -5 to 5. All values of <span class="math notranslate nohighlight">\(\lambda\)</span> are considered and the optimal value for the given data is estimated from the training data; The “optimal value” is the one which results in the best transformation to an approximate normal distribution. The transformation of the response <span class="math notranslate nohighlight">\(Y\)</span> has the form:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}
 \begin{equation} 
 y(\lambda) =
\begin{cases}
   \frac{Y^\lambda-1}{\lambda}, &amp; \text{if}\ \lambda \neq 0 \\
   \log\left(Y\right), &amp; \text{if}\ \lambda = 0.
\end{cases}
\end{equation}
\end{split}\]</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If your response has negative values, the Yeo-Johnson transformation is very similar to the Box-Cox but does not require the input variables to be strictly positive.</p>
</div>
<p>Below illustrates that the log transformation and Box Cox transformation both do about equally well in transforming <code class="docutils literal notranslate"><span class="pre">Sale_Price</span></code> to look more normally distributed.</p>
<p><img alt="" src="../_images/engineering-distribution-comparison-1.png" /></p>
<p>In Python we use <code class="docutils literal notranslate"><span class="pre">TransformedTargetRegressor()</span></code> to build a plan for target engineering. This will not return the actual log/box-cox transformed values but, rather, a blueprint to be applied later. In this example we are simply building an object that will apply a Box-Cox transformation to the target variable when we fit our model.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>There is a <code class="docutils literal notranslate"><span class="pre">sklearn.preprocessing.power_transform</span></code> function that can be applied to immediately transform an array. However, later in this module we’ll discuss the idea of data leakage and how important it is to create isolated pre-processing steps.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tt</span> <span class="o">=</span> <span class="n">TransformedTargetRegressor</span><span class="p">(</span><span class="n">transformer</span><span class="o">=</span><span class="n">PowerTransformer</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;box-cox&#39;</span><span class="p">))</span>
<span class="n">tt</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TransformedTargetRegressor(transformer=PowerTransformer(method=&#39;box-cox&#39;))
</pre></div>
</div>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="dealing-with-missingness">
<h1>Dealing with missingness<a class="headerlink" href="#dealing-with-missingness" title="Permalink to this headline">¶</a></h1>
<p>Data quality is an important issue for any project involving analyzing data. Data quality issues deserve an entire book in their own right, and a good reference is <a class="reference external" href="https://github.com/Quartz/bad-data-guide">The Quartz guide to bad data</a>. One of the most common data quality concerns you will run into is missing values.</p>
<p>Data can be missing for many different reasons; however, these reasons are usually lumped into two categories: <em>informative missingness</em> <span id="id5">[<a class="reference internal" href="#id15">KJ13</a>]</span> and <em>missingness at random</em> <span id="id6">[<a class="reference internal" href="#id31">LR14</a>]</span>. Informative missingness implies a structural cause for the missing value that can provide insight in its own right; whether this be deficiencies in how the data was collected or abnormalities in the observational environment.  Missingness at random implies that missing values occur independent of the data collection process<a class="footnote-reference brackets" href="#missing-at-random" id="id7">1</a>.</p>
<p>The category that drives missing values will determine how you handle them.  For example, we may give values that are driven by informative missingness their own category (e.g., <code class="docutils literal notranslate"><span class="pre">&quot;None&quot;</span></code>) as their unique value may affect predictive performance.  Whereas values that are missing at random may deserve deletion<a class="footnote-reference brackets" href="#missing-at-random2" id="id8">2</a> or imputation.</p>
<p>Furthermore, different machine learning models handle missingness differently.  Most algorithms cannot handle missingness (e.g., generalized linear models and their cousins, neural networks, and support vector machines) and, therefore, require them to be dealt with beforehand.  A few models (mainly tree-based), have built-in procedures to deal with missing values.  However, since the modeling process involves comparing and contrasting multiple models to identify the optimal one, you will want to handle missing values prior to applying any models so that your algorithms are based on the same data quality assumptions.</p>
<div class="section" id="visualizing-missing-values">
<h2>Visualizing missing values<a class="headerlink" href="#visualizing-missing-values" title="Permalink to this headline">¶</a></h2>
<p>It is important to understand the distribution of missing values (i.e., <code class="docutils literal notranslate"><span class="pre">NA</span></code>) in any data set. So far, we have been using a pre-processed version of the Ames housing data set. However, if we use the <a class="reference external" href="https://rdrr.io/cran/AmesHousing/man/ames_raw.html">raw Ames housing data</a>, there are actually <code class="docutils literal notranslate"><span class="pre">r</span> <span class="pre">scales::comma(sum(is.na(AmesHousing::ames_raw)))</span></code> missing values—there is at least one missing value in each row of the original data! Good visualizations can help us understand patterns in the missing data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ames_raw</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../data/ames_raw.csv&quot;</span><span class="p">)</span>

<span class="c1"># count total missing values</span>
<span class="n">ames_raw</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>13997
</pre></div>
</div>
</div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Check out the <a class="reference external" href="https://github.com/ResidentMario/missingno">missingno</a> package to see all the great ways to to visualize missing data!</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># can you identify patterns of missing data</span>
<span class="c1"># missingness is represented with white</span>
<span class="n">msno</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">ames_raw</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="nb">filter</span><span class="o">=</span><span class="s2">&quot;bottom&quot;</span><span class="p">,</span> <span class="n">sort</span><span class="o">=</span><span class="s2">&quot;ascending&quot;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:&gt;
</pre></div>
</div>
<img alt="../_images/feature_eng_8_1.png" src="../_images/feature_eng_8_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># which features have most missing?</span>
<span class="c1"># this chart shows the number of observations so small bars (i.e. Pool QC)</span>
<span class="c1"># represent very few observed values (lots of missingness)</span>
<span class="n">msno</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">ames_raw</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="nb">filter</span><span class="o">=</span><span class="s2">&quot;bottom&quot;</span><span class="p">,</span> <span class="n">sort</span><span class="o">=</span><span class="s2">&quot;ascending&quot;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:&gt;
</pre></div>
</div>
<img alt="../_images/feature_eng_9_1.png" src="../_images/feature_eng_9_1.png" />
</div>
</div>
</div>
<div class="section" id="imputation">
<h2>Imputation<a class="headerlink" href="#imputation" title="Permalink to this headline">¶</a></h2>
<p><em>Imputation</em> is the process of replacing a missing value with a substituted, “best guess” value. Imputation should be one of the first feature engineering steps you take as it will affect any downstream pre-processing<a class="footnote-reference brackets" href="#imputation1" id="id9">3</a>.</p>
<div class="section" id="estimated-statistic">
<h3>Estimated statistic<a class="headerlink" href="#estimated-statistic" title="Permalink to this headline">¶</a></h3>
<p>An elementary approach to imputing missing values for a feature is to compute descriptive statistics such as the mean, median, or mode (for categorical) and use that value to replace <code class="docutils literal notranslate"><span class="pre">NA</span></code>s. Although computationally efficient, this approach does not consider any other attributes for a given observation when imputing (e.g., a female patient that is 63 inches tall may have her weight imputed as 175 lbs since that is the average weight across all observations which contains 65% males that have an average a height of 70 inches).</p>
<p>An alternative is to use grouped statistics to capture expected values for observations that fall into similar groups. However, this becomes infeasible for larger data sets.  Modeling imputation can automate this process for you and the two most common methods include K-nearest neighbor and tree-based imputation, which are discussed next.</p>
<p>However, it is important to remember that imputation should be performed <strong>within the resampling process</strong> and as your data set gets larger, repeated model-based imputation can compound the computational demands.  Thus, you must weigh the pros and cons of the two approaches.</p>
<p>The following code snippet shows how to impute different features with the median value of a given feature.</p>
<p>In Python, the <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html"><code class="docutils literal notranslate"><span class="pre">SimpleImputer</span></code></a> can be used to apply an imputation to all features. However, if you only want to apply imputation to a subset of features then you would use <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html"><code class="docutils literal notranslate"><span class="pre">ColumnTransformer</span></code></a> in addition to the <code class="docutils literal notranslate"><span class="pre">SimpleImputer</span></code> strategy.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>These imputers do not yet perform imputation. At the end of this module we will show you how all these pieces of the feature engineering come together in our ML modeling pipeline.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># median imputation to all features</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;median&#39;</span><span class="p">)</span>

<span class="c1"># median imputation to just numeric predictors</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">([(</span><span class="s2">&quot;num_imp&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">selector</span><span class="p">(</span><span class="n">dtype_include</span><span class="o">=</span><span class="s2">&quot;number&quot;</span><span class="p">))])</span>

<span class="c1"># median imputation to 1 or more features</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">([(</span><span class="s2">&quot;num_imp&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">selector</span><span class="p">(</span><span class="s2">&quot;Gr_Liv_Area&quot;</span><span class="p">))])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="k-nearest-neighbor">
<h3>K-nearest neighbor<a class="headerlink" href="#k-nearest-neighbor" title="Permalink to this headline">¶</a></h3>
<p><em>K</em>-nearest neighbor (KNN) imputes values by identifying observations with missing values, then identifying other observations that are most similar based on the other available features, and using the values from these nearest neighbor observations to impute missing values.</p>
<p>We discuss KNN for predictive modeling in a later module; the imputation application works in a similar manner.  In KNN imputation, the missing value for a given observation is treated as the targeted response and is predicted based on the average (for quantitative values) or the mode (for qualitative values) of the <em>k</em> nearest neighbors.</p>
<p>As discussed in the KNN modeling module, if all features are quantitative then standard Euclidean distance is commonly used as the distance metric to identify the <em>k</em> neighbors and when there is a mixture of quantitative and qualitative features then Gower’s distance <span id="id10">[<a class="reference internal" href="#id32">Gow71</a>]</span> can be used. KNN imputation is best used on small to moderate sized data sets as it becomes computationally burdensome with larger data sets <span id="id11">[<a class="reference internal" href="#id27">KJ19</a>]</span>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>As we saw in the last module, <strong>k</strong> is a tunable hyperparameter. Suggested values for imputation are 5–10 <span id="id12">[<a class="reference internal" href="#id27">KJ19</a>]</span>.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">knn_imp</span> <span class="o">=</span> <span class="n">KNNImputer</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Read more about Scikit-Learn’s imputation options <a class="reference external" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute">here</a>.</p>
</div>
<p>The plot below illustrates the differences between mean and KNN-based imputation on the raw Ames housing data. The red points represent actual values which were removed and made missing and the blue points represent the imputed values. Estimated statistic imputation methods (i.e. mean, median) merely predict the same value for each observation and can reduce the signal between a feature and the response; whereas KNN imputation procedures tend to maintain the feature distribution and relationship.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">gridspec</span>
<span class="kn">from</span> <span class="nn">mizani</span> <span class="kn">import</span> <span class="n">formatters</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">module</span><span class="o">=</span><span class="s1">&#39;plotnine&#39;</span><span class="p">)</span>

<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">sample_index</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">ames</span><span class="o">.</span><span class="n">index</span><span class="p">),</span> <span class="mi">50</span><span class="p">)</span>

<span class="c1"># identify actuals</span>
<span class="n">actuals</span> <span class="o">=</span> <span class="n">ames</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">sample_index</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">p1</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">ggplot</span><span class="p">()</span>
    <span class="o">+</span> <span class="n">geom_point</span><span class="p">(</span><span class="n">ames</span><span class="p">,</span> <span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;Gr_Liv_Area&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Sale_Price&quot;</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">geom_point</span><span class="p">(</span><span class="n">actuals</span><span class="p">,</span> <span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;Gr_Liv_Area&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Sale_Price&quot;</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">scale_x_log10</span><span class="p">(</span><span class="n">limits</span><span class="o">=</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">4000</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">scale_y_log10</span><span class="p">(</span>
        <span class="n">limits</span><span class="o">=</span><span class="p">(</span><span class="mi">50000</span><span class="p">,</span> <span class="mi">600000</span><span class="p">),</span>
        <span class="n">labels</span><span class="o">=</span><span class="n">formatters</span><span class="o">.</span><span class="n">currency_format</span><span class="p">(</span><span class="n">digits</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">big_mark</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="o">+</span> <span class="n">ggtitle</span><span class="p">(</span><span class="s2">&quot;Actual values&quot;</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># mean imputed</span>
<span class="n">mean_impute</span> <span class="o">=</span> <span class="n">actuals</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">mean_impute</span><span class="p">[</span><span class="s1">&#39;Gr_Liv_Area&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">actuals</span><span class="p">[</span><span class="s1">&#39;Gr_Liv_Area&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">p2</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">ggplot</span><span class="p">()</span>
    <span class="o">+</span> <span class="n">geom_point</span><span class="p">(</span><span class="n">actuals</span><span class="p">,</span> <span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;Gr_Liv_Area&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Sale_Price&quot;</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">geom_point</span><span class="p">(</span><span class="n">mean_impute</span><span class="p">,</span> <span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;Gr_Liv_Area&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Sale_Price&quot;</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">scale_x_log10</span><span class="p">(</span><span class="n">limits</span><span class="o">=</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">4000</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">scale_y_log10</span><span class="p">(</span>
        <span class="n">limits</span><span class="o">=</span><span class="p">(</span><span class="mi">50000</span><span class="p">,</span> <span class="mi">600000</span><span class="p">),</span>
        <span class="n">labels</span><span class="o">=</span><span class="n">formatters</span><span class="o">.</span><span class="n">currency_format</span><span class="p">(</span><span class="n">digits</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">big_mark</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="o">+</span> <span class="n">ggtitle</span><span class="p">(</span><span class="s2">&quot;Mean imputation&quot;</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># knn imputed</span>
<span class="n">knn_imp</span> <span class="o">=</span> <span class="n">KNNImputer</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="n">ames_numeric</span> <span class="o">=</span> <span class="n">ames</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">number</span><span class="p">])</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">ames_numeric</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">sample_index</span><span class="p">,</span> <span class="s1">&#39;Gr_Liv_Area&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
<span class="n">knn_impute</span> <span class="o">=</span> <span class="n">knn_imp</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">ames_numeric</span><span class="p">)</span>
<span class="n">knn_impute</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">knn_impute</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">ames_numeric</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">sample_index</span><span class="p">]</span>
<span class="n">p3</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">ggplot</span><span class="p">()</span>
    <span class="o">+</span> <span class="n">geom_point</span><span class="p">(</span><span class="n">actuals</span><span class="p">,</span> <span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;Gr_Liv_Area&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Sale_Price&quot;</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">geom_point</span><span class="p">(</span><span class="n">knn_impute</span><span class="p">,</span> <span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;Gr_Liv_Area&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Sale_Price&quot;</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">scale_x_log10</span><span class="p">(</span><span class="n">limits</span><span class="o">=</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">4000</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">scale_y_log10</span><span class="p">(</span>
        <span class="n">limits</span><span class="o">=</span><span class="p">(</span><span class="mi">50000</span><span class="p">,</span> <span class="mi">600000</span><span class="p">),</span>
        <span class="n">labels</span><span class="o">=</span><span class="n">formatters</span><span class="o">.</span><span class="n">currency_format</span><span class="p">(</span><span class="n">digits</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">big_mark</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="o">+</span> <span class="n">ggtitle</span><span class="p">(</span><span class="s2">&quot;KNN imputation&quot;</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Empty plotnine figure to place the subplots on. Needs junk data (for backend &quot;copy&quot; reasons).</span>

<span class="n">fig</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">ggplot</span><span class="p">()</span>
    <span class="o">+</span> <span class="n">geom_blank</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">ames</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">theme_void</span><span class="p">()</span>
    <span class="o">+</span> <span class="n">theme</span><span class="p">(</span><span class="n">figure_size</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="c1"># such a hack it&#39;s embarrassing!</span>
    <span class="o">+</span> <span class="n">ggtitle</span><span class="p">((</span>
        <span class="s2">&quot;    &quot;</span> <span class="o">+</span>
        <span class="s2">&quot;(1) Actual values                                              &quot;</span> <span class="o">+</span>
        <span class="s2">&quot;(2) Mean imputation                                         &quot;</span> <span class="o">+</span>
        <span class="s2">&quot;(3) KNN imputation                          &quot;</span>
        <span class="p">))</span>
    <span class="p">)</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>

<span class="c1"># Create gridspec for adding subpanels to the blank figure</span>
<span class="n">gs</span> <span class="o">=</span> <span class="n">gridspec</span><span class="o">.</span><span class="n">GridSpec</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax2</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax3</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>

<span class="c1"># Add subplots to the figure</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">p1</span><span class="o">.</span><span class="n">_draw_using_figure</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="p">[</span><span class="n">ax1</span><span class="p">])</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">p2</span><span class="o">.</span><span class="n">_draw_using_figure</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="p">[</span><span class="n">ax2</span><span class="p">])</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">p3</span><span class="o">.</span><span class="n">_draw_using_figure</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="p">[</span><span class="n">ax3</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/feature_eng_16_0.png" src="../_images/feature_eng_16_0.png" />
</div>
</div>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="feature-filtering">
<h1>Feature filtering<a class="headerlink" href="#feature-filtering" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="numeric-feature-engineering">
<h1>Numeric feature engineering<a class="headerlink" href="#numeric-feature-engineering" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="categorical-feature-engineering">
<h1>Categorical feature engineering<a class="headerlink" href="#categorical-feature-engineering" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="dimension-reduction">
<h1>Dimension reduction<a class="headerlink" href="#dimension-reduction" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="proper-implementation">
<h1>Proper implementation<a class="headerlink" href="#proper-implementation" title="Permalink to this headline">¶</a></h1>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="references">
<h1>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h1>
<p id="id13"><dl class="citation">
<dt class="label" id="id25"><span class="brackets">BBBKegl11</span></dt>
<dd><p>James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper-parameter optimization. <em>Advances in neural information processing systems</em>, 2011.</p>
</dd>
<dt class="label" id="id24"><span class="brackets">BB12</span></dt>
<dd><p>James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. <em>Journal of Machine Learning Research</em>, 13(Feb):281–305, 2012.</p>
</dd>
<dt class="label" id="id29"><span class="brackets"><a class="fn-backref" href="#id4">BC64</a></span></dt>
<dd><p>George EP Box and David R Cox. An analysis of transformations. <em>Journal of the Royal Statistical Society. Series B (Methodological)</em>, pages 211–252, 1964.</p>
</dd>
<dt class="label" id="id23"><span class="brackets"><a class="fn-backref" href="#id1">B+01</a></span></dt>
<dd><p>Leo Breiman and others. Statistical modeling: the two cultures (with comments and a rejoinder by the author). <em>Statistical Science</em>, 16(3):199–231, 2001.</p>
</dd>
<dt class="label" id="id30"><span class="brackets"><a class="fn-backref" href="#id4">CR81</a></span></dt>
<dd><p>Raymond J Carroll and David Ruppert. On prediction and the power transformation family. <em>Biometrika</em>, 68(3):609–615, 1981.</p>
</dd>
<dt class="label" id="id16"><span class="brackets">CBHK02</span></dt>
<dd><p>Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. <em>Journal of Artificial Intelligence Research</em>, 16:321–357, 2002.</p>
</dd>
<dt class="label" id="id21"><span class="brackets">DH+97</span></dt>
<dd><p>Anthony Christopher Davison, David Victor Hinkley, and others. <em>Bootstrap Methods and their Application</em>. Volume 1. Cambridge University Press, 1997.</p>
</dd>
<dt class="label" id="id20"><span class="brackets">Efr83</span></dt>
<dd><p>Bradley Efron. Estimating the error rate of a prediction rule: improvement on cross-validation. <em>Journal of the American Statistical Association</em>, 78(382):316–331, 1983.</p>
</dd>
<dt class="label" id="id22"><span class="brackets">ET97</span></dt>
<dd><p>Bradley Efron and Robert Tibshirani. Improvements on cross-validation: the 632+ bootstrap method. <em>Journal of the American Statistical Association</em>, 92(438):548–560, 1997.</p>
</dd>
<dt class="label" id="id19"><span class="brackets">FHT01</span></dt>
<dd><p>Jerome Friedman, Trevor Hastie, and Robert Tibshirani. <em>The Elements of Statistical Learning</em>. Volume 1. Springer Series in Statistics New York, NY, USA:, 2001.</p>
</dd>
<dt class="label" id="id32"><span class="brackets"><a class="fn-backref" href="#id10">Gow71</a></span></dt>
<dd><p>John C Gower. A general coefficient of similarity and some of its properties. <em>Biometrics</em>, pages 857–871, 1971.</p>
</dd>
<dt class="label" id="id18"><span class="brackets">HBM03</span></dt>
<dd><p>Douglas M Hawkins, Subhash C Basak, and Denise Mills. Assessing model fit by cross-validation. <em>Journal of Chemical Information and Computer Sciences</em>, 43(2):579–586, 2003.</p>
</dd>
<dt class="label" id="id26"><span class="brackets">Kim09</span></dt>
<dd><p>Ji-Hyun Kim. Estimating classification error rate: repeated cross-validation, repeated hold-out and bootstrap. <em>Computational Statistics &amp; Data Analysis</em>, 53(11):3735–3745, 2009.</p>
</dd>
<dt class="label" id="id15"><span class="brackets"><a class="fn-backref" href="#id5">KJ13</a></span></dt>
<dd><p>Max Kuhn and Kjell Johnson. <em>Applied Predictive Modeling</em>. Volume 26. Springer, 2013.</p>
</dd>
<dt class="label" id="id27"><span class="brackets">KJ19</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id11">2</a>,<a href="#id12">3</a>)</span></dt>
<dd><p>Max Kuhn and Kjell Johnson. <em>Feature Engineering and Selection: A Practical Approach for Predictive Models</em>. Chapman &amp; Hall/CRC, 2019.</p>
</dd>
<dt class="label" id="id31"><span class="brackets">LR14</span><span class="fn-backref">(<a href="#id6">1</a>,<a href="#id33">2</a>)</span></dt>
<dd><p>Roderick JA Little and Donald B Rubin. <em>Statistical Analysis with Missing Data</em>. Volume 333. John Wiley &amp; Sons, 2014.</p>
</dd>
<dt class="label" id="id17"><span class="brackets">MSP05</span></dt>
<dd><p>Annette M Molinaro, Richard Simon, and Ruth M Pfeiffer. Prediction error estimation: a comparison of resampling methods. <em>Bioinformatics</em>, 21(15):3301–3307, 2005.</p>
</dd>
<dt class="label" id="id14"><span class="brackets">Wol96</span></dt>
<dd><p>David H Wolpert. The lack of a priori distinctions between learning algorithms. <em>Neural Computation</em>, 8(7):1341–1390, 1996.</p>
</dd>
<dt class="label" id="id28"><span class="brackets"><a class="fn-backref" href="#id3">ZC18</a></span></dt>
<dd><p>Alice Zheng and Amanda Casari. <em>Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists</em>. O'Reilly Media, Inc., 2018.</p>
</dd>
</dl>
</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="missing-at-random"><span class="brackets"><a class="fn-backref" href="#id7">1</a></span></dt>
<dd><p><span id="id33">[<a class="reference internal" href="#id31">LR14</a>]</span> discuss two different kinds of missingness at random; however, we combine them for simplicity as their nuanced differences are distinguished between the two in practice.</p>
</dd>
<dt class="label" id="missing-at-random2"><span class="brackets"><a class="fn-backref" href="#id8">2</a></span></dt>
<dd><p>If your data set is large, deleting missing observations that have missing values at random rarely impacts predictive performance. However, as your data sets get smaller, preserving observations is critical and alternative solutions should be explored.</p>
</dd>
<dt class="label" id="imputation1"><span class="brackets"><a class="fn-backref" href="#id9">3</a></span></dt>
<dd><p>For example, standardizing numeric features will include the imputed numeric values in the calculation and one-hot encoding will include the imputed categorical value.</p>
</dd>
</dl>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./fundamentals"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="modeling_process.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Modeling Process</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../supervised_learning/content.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Content in Jupyter Book</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Bradley C. Boehmke<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>